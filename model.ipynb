{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b5df43",
   "metadata": {},
   "source": [
    "# ðŸš€ CYCLE-TO-PEAK PERFORMANCE PREDICTOR\n",
    "## Implementation of the Three-Pillar Approach\n",
    "\n",
    "This notebook implements:\n",
    "- **Pillar 1**: Training Cycle Calculation (J-days)\n",
    "- **Pillar 2**: Advanced Feature Engineering (GPS + Wyscout)\n",
    "- **Pillar 3**: AI-Powered Performance Prediction (LSTM)\n",
    "\n",
    "Each pillar will be tested before proceeding to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2cbb5",
   "metadata": {},
   "source": [
    "---\n",
    "# PILLAR 1: Training Cycle Calculation\n",
    "**Goal**: Map each training session to its J-day (days until next match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52ffd9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PILLAR 1: Training Cycle Calculation\n",
      "============================================================\n",
      "\n",
      "âœ“ Loaded GPS Match data: (2108, 812)\n",
      "âœ“ Loaded GPS Training data: (7903, 810)\n",
      "\n",
      "âœ“ Date range (Match): 2025-07-30 00:00:00 to 2025-11-06 00:00:00\n",
      "âœ“ Date range (Training): 2025-07-15 00:00:00 to 2025-11-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PILLAR 1: TRAINING CYCLE CALCULATION\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PILLAR 1: Training Cycle Calculation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load data\n",
    "DATA_DIR = Path(\"hackathon_Data/hackathon/Data After Extraction/CSV\")\n",
    "\n",
    "def safe_read(path, encoding='utf-8'):\n",
    "    for enc in [encoding, 'latin1', 'cp1252']:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "# Read GPS data\n",
    "gps_match = safe_read(DATA_DIR / \"matchs_gps.csv\")\n",
    "gps_train = safe_read(DATA_DIR / \"training_gps.csv\")\n",
    "\n",
    "# Normalize column names\n",
    "def normalize_cols(df):\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    return df\n",
    "\n",
    "gps_match = normalize_cols(gps_match)\n",
    "gps_train = normalize_cols(gps_train)\n",
    "\n",
    "# Parse dates\n",
    "gps_match['date'] = pd.to_datetime(gps_match['date'], errors='coerce')\n",
    "gps_train['date'] = pd.to_datetime(gps_train['date'], errors='coerce')\n",
    "\n",
    "print(f\"\\nâœ“ Loaded GPS Match data: {gps_match.shape}\")\n",
    "print(f\"âœ“ Loaded GPS Training data: {gps_train.shape}\")\n",
    "print(f\"\\nâœ“ Date range (Match): {gps_match['date'].min()} to {gps_match['date'].max()}\")\n",
    "print(f\"âœ“ Date range (Training): {gps_train['date'].min()} to {gps_train['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cc92995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Calendar created: (2041, 4)\n",
      "âœ“ Unique players: 42\n",
      "âœ“ Event type distribution:\n",
      "event_type\n",
      "Training    1717\n",
      "Match        324\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“‹ Sample calendar:\n",
      "              player_name team_name       date event_type\n",
      "0   Abdelmounaim Boutouil  AL HAZEM 2025-09-07   Training\n",
      "1   Abdelmounaim Boutouil  AL HAZEM 2025-09-08   Training\n",
      "2   Abdelmounaim Boutouil  AL HAZEM 2025-09-09   Training\n",
      "3   Abdelmounaim Boutouil  AL HAZEM 2025-09-10   Training\n",
      "4   Abdelmounaim Boutouil  AL HAZEM 2025-09-14   Training\n",
      "5   Abdelmounaim Boutouil  AL HAZEM 2025-09-15   Training\n",
      "6   Abdelmounaim Boutouil  AL HAZEM 2025-09-16   Training\n",
      "7   Abdelmounaim Boutouil  AL HAZEM 2025-09-17   Training\n",
      "8   Abdelmounaim Boutouil  AL HAZEM 2025-09-18   Training\n",
      "9   Abdelmounaim Boutouil  AL HAZEM 2025-09-19   Training\n",
      "10  Abdelmounaim Boutouil  AL HAZEM 2025-09-21   Training\n",
      "11  Abdelmounaim Boutouil  AL HAZEM 2025-09-22   Training\n",
      "12  Abdelmounaim Boutouil  AL HAZEM 2025-09-24   Training\n",
      "13  Abdelmounaim Boutouil  AL HAZEM 2025-09-25   Training\n",
      "14  Abdelmounaim Boutouil  AL HAZEM 2025-10-02   Training\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1: Create unified GPS calendar with event types\n",
    "gps_match['event_type'] = 'Match'\n",
    "gps_train['event_type'] = 'Training'\n",
    "\n",
    "# Combine both datasets\n",
    "gps_all = pd.concat([gps_match, gps_train], ignore_index=True)\n",
    "\n",
    "# Create base calendar\n",
    "calendar = gps_all[['name', 'team_name', 'date', 'event_type']].copy()\n",
    "calendar.columns = ['player_name', 'team_name', 'date', 'event_type']\n",
    "calendar = calendar.dropna(subset=['player_name', 'date'])\n",
    "calendar = calendar.drop_duplicates()\n",
    "calendar = calendar.sort_values(['player_name', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nâœ“ Calendar created: {calendar.shape}\")\n",
    "print(f\"âœ“ Unique players: {calendar['player_name'].nunique()}\")\n",
    "print(f\"âœ“ Event type distribution:\")\n",
    "print(calendar['event_type'].value_counts())\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nðŸ“‹ Sample calendar:\")\n",
    "print(calendar.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bff7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ J-days calculated!\n",
      "\n",
      "ðŸ“Š J-label distribution:\n",
      "J_label\n",
      "J-1      309\n",
      "J-10      14\n",
      "J-11      33\n",
      "J-12      39\n",
      "J-13      38\n",
      "J-14      20\n",
      "J-15      26\n",
      "J-16      15\n",
      "J-17       5\n",
      "J-18       5\n",
      "J-19       6\n",
      "J-2      312\n",
      "J-20       6\n",
      "J-21       4\n",
      "J-22       1\n",
      "J-23       1\n",
      "J-27       1\n",
      "J-28       2\n",
      "J-29       1\n",
      "J-3      212\n",
      "J-30       1\n",
      "J-31       2\n",
      "J-32       1\n",
      "J-33       2\n",
      "J-34       2\n",
      "J-35       2\n",
      "J-36       2\n",
      "J-37       1\n",
      "J-4      176\n",
      "J-5      137\n",
      "J-6       55\n",
      "J-7       25\n",
      "J-8       36\n",
      "J-9       31\n",
      "J-Day    291\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“‹ Example training cycle for one player:\n",
      "              player_name       date event_type J_label  days_to_next_match\n",
      "0   Abdelmounaim Boutouil 2025-09-07   Training    J-30                30.0\n",
      "1   Abdelmounaim Boutouil 2025-09-08   Training    J-29                29.0\n",
      "2   Abdelmounaim Boutouil 2025-09-09   Training    J-28                28.0\n",
      "3   Abdelmounaim Boutouil 2025-09-10   Training    J-27                27.0\n",
      "4   Abdelmounaim Boutouil 2025-09-14   Training    J-23                23.0\n",
      "5   Abdelmounaim Boutouil 2025-09-15   Training    J-22                22.0\n",
      "6   Abdelmounaim Boutouil 2025-09-16   Training    J-21                21.0\n",
      "7   Abdelmounaim Boutouil 2025-09-17   Training    J-20                20.0\n",
      "8   Abdelmounaim Boutouil 2025-09-18   Training    J-19                19.0\n",
      "9   Abdelmounaim Boutouil 2025-09-19   Training    J-18                18.0\n",
      "10  Abdelmounaim Boutouil 2025-09-21   Training    J-16                16.0\n",
      "11  Abdelmounaim Boutouil 2025-09-22   Training    J-15                15.0\n",
      "12  Abdelmounaim Boutouil 2025-09-24   Training    J-13                13.0\n",
      "13  Abdelmounaim Boutouil 2025-09-25   Training    J-12                12.0\n",
      "14  Abdelmounaim Boutouil 2025-10-02   Training     J-5                 5.0\n",
      "15  Abdelmounaim Boutouil 2025-10-03   Training     J-4                 4.0\n",
      "16  Abdelmounaim Boutouil 2025-10-04   Training     J-3                 3.0\n",
      "17  Abdelmounaim Boutouil 2025-10-05   Training     J-2                 2.0\n",
      "18  Abdelmounaim Boutouil 2025-10-06   Training     J-1                 1.0\n",
      "19  Abdelmounaim Boutouil 2025-10-07      Match   J-Day                18.0\n"
     ]
    }
   ],
   "source": [
    "# Step 1.2: Calculate J-days (Training Cycle)\n",
    "def calculate_j_days(df):\n",
    "    \"\"\"\n",
    "    For each player, calculate:\n",
    "    - next_match_date: date of the next match\n",
    "    - days_to_next_match: days until next match\n",
    "    - J_label: J-Day, J-1, J-2, etc.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['player_name', 'date']).copy()\n",
    "    results = []\n",
    "    \n",
    "    for player, grp in df.groupby('player_name'):\n",
    "        grp = grp.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Find match dates for this player\n",
    "        match_dates = grp[grp['event_type'] == 'Match']['date'].values\n",
    "        \n",
    "        # For each row, find the next match\n",
    "        next_match = []\n",
    "        for date in grp['date']:\n",
    "            future_matches = match_dates[match_dates > date]\n",
    "            if len(future_matches) > 0:\n",
    "                next_match.append(pd.Timestamp(future_matches[0]))\n",
    "            else:\n",
    "                next_match.append(pd.NaT)\n",
    "        \n",
    "        grp['next_match_date'] = next_match\n",
    "        \n",
    "        # Calculate days to next match\n",
    "        grp['days_to_next_match'] = (grp['next_match_date'] - grp['date']).dt.days\n",
    "        \n",
    "        # Create J-label\n",
    "        def make_j_label(row):\n",
    "            if pd.isna(row['next_match_date']):\n",
    "                return None\n",
    "            if row['event_type'] == 'Match':\n",
    "                return 'J-Day'\n",
    "            if row['days_to_next_match'] > 0:\n",
    "                return f\"J-{int(row['days_to_next_match'])}\"\n",
    "            return None\n",
    "        \n",
    "        grp['J_label'] = grp.apply(make_j_label, axis=1)\n",
    "        results.append(grp)\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Apply J-day calculation\n",
    "calendar = calculate_j_days(calendar)\n",
    "\n",
    "print(\"\\nâœ“ J-days calculated!\")\n",
    "print(f\"\\nðŸ“Š J-label distribution:\")\n",
    "print(calendar['J_label'].value_counts().sort_index())\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nðŸ“‹ Example training cycle for one player:\")\n",
    "sample_player = calendar['player_name'].iloc[0]\n",
    "sample = calendar[calendar['player_name'] == sample_player].head(20)\n",
    "print(sample[['player_name', 'date', 'event_type', 'J_label', 'days_to_next_match']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "529ee733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PILLAR 1 VALIDATION\n",
      "============================================================\n",
      "\n",
      "âœ“ Training sessions with J-labels: 1523 / 1717\n",
      "âœ“ Match days labeled as J-Day: 291 / 324\n",
      "\n",
      "ðŸ“‹ Typical training cycle (J-6 to J-Day):\n",
      "              player_name       date event_type J_label\n",
      "14  Abdelmounaim Boutouil 2025-10-02   Training     J-5\n",
      "15  Abdelmounaim Boutouil 2025-10-03   Training     J-4\n",
      "16  Abdelmounaim Boutouil 2025-10-04   Training     J-3\n",
      "17  Abdelmounaim Boutouil 2025-10-05   Training     J-2\n",
      "18  Abdelmounaim Boutouil 2025-10-06   Training     J-1\n",
      "19  Abdelmounaim Boutouil 2025-10-07      Match   J-Day\n",
      "26  Abdelmounaim Boutouil 2025-10-22   Training     J-3\n",
      "27  Abdelmounaim Boutouil 2025-10-23   Training     J-2\n",
      "28  Abdelmounaim Boutouil 2025-10-24   Training     J-1\n",
      "29  Abdelmounaim Boutouil 2025-10-25      Match   J-Day\n",
      "\n",
      "ðŸ’¾ Saved: data\\processed\\pillar1_calendar_j_days.csv\n",
      "\n",
      "âœ… PILLAR 1 COMPLETE - Ready for Pillar 2\n"
     ]
    }
   ],
   "source": [
    "# Step 1.3: Validate Pillar 1\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PILLAR 1 VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: Are there training days with J-labels?\n",
    "training_with_j = calendar[(calendar['event_type'] == 'Training') & (calendar['J_label'].notna())]\n",
    "print(f\"\\nâœ“ Training sessions with J-labels: {len(training_with_j)} / {len(calendar[calendar['event_type'] == 'Training'])}\")\n",
    "\n",
    "# Check 2: Are match days labeled as J-Day?\n",
    "match_j_day = calendar[(calendar['event_type'] == 'Match') & (calendar['J_label'] == 'J-Day')]\n",
    "print(f\"âœ“ Match days labeled as J-Day: {len(match_j_day)} / {len(calendar[calendar['event_type'] == 'Match'])}\")\n",
    "\n",
    "# Check 3: Show typical training cycle\n",
    "print(\"\\nðŸ“‹ Typical training cycle (J-6 to J-Day):\")\n",
    "cycle_example = calendar[calendar['J_label'].isin(['J-6', 'J-5', 'J-4', 'J-3', 'J-2', 'J-1', 'J-Day'])].head(10)\n",
    "print(cycle_example[['player_name', 'date', 'event_type', 'J_label']])\n",
    "\n",
    "# Save Pillar 1 output\n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "calendar.to_csv(OUT_DIR / \"pillar1_calendar_j_days.csv\", index=False)\n",
    "print(f\"\\nðŸ’¾ Saved: {OUT_DIR / 'pillar1_calendar_j_days.csv'}\")\n",
    "\n",
    "print(\"\\nâœ… PILLAR 1 COMPLETE - Ready for Pillar 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387a95f",
   "metadata": {},
   "source": [
    "---\n",
    "# PILLAR 2: Advanced Feature Engineering\n",
    "**Goal**: Extract physical (GPS) and technical (Wyscout) features for each player-date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55dba98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PILLAR 2: Feature Engineering\n",
      "============================================================\n",
      "\n",
      "[Step 2.1] Extracting GPS physical metrics...\n",
      "âœ“ Extracted 18 GPS features\n",
      "âœ“ GPS features shape: (10011, 22)\n",
      "\n",
      "ðŸ“Š Available GPS metrics:\n",
      "  - total_distance\n",
      "  - total_duration\n",
      "  - high_speed_distance\n",
      "  - sprint_distance\n",
      "  - distance_above_20\n",
      "  - distance_above_25\n",
      "  - total_player_load\n",
      "  - player_load_per_min\n",
      "  - avg_player_load\n",
      "  - high_accel_efforts\n",
      "  - high_decel_efforts\n",
      "  - total_accel_decel\n",
      "  - explosive_efforts\n",
      "  - high_metabolic_distance\n",
      "  - equivalent_distance\n",
      "  - avg_hr\n",
      "  - max_hr\n",
      "  - hr_exertion\n",
      "\n",
      "ðŸ“‹ Sample GPS features:\n",
      "            player_name team_name       date event_type  total_distance  \\\n",
      "0    Yousef Al-Shammari  AL HAZEM 2025-08-08      Match      1410.80823   \n",
      "1  Abdullah Al-Shanqiti  AL HAZEM 2025-09-23      Match      1234.82007   \n",
      "2    Yousef Al-Shammari  AL HAZEM 2025-08-08      Match      7038.72546   \n",
      "\n",
      "  total_duration  high_speed_distance  sprint_distance  distance_above_20  \\\n",
      "0       00:15:00                 0.00                0               5.23   \n",
      "1       00:15:00                 0.00                0               0.00   \n",
      "2       01:12:27                26.94                1             139.86   \n",
      "\n",
      "   distance_above_25  ...  avg_player_load  high_accel_efforts  \\\n",
      "0               0.00  ...        124.75719                   0   \n",
      "1               0.00  ...        116.69658                   0   \n",
      "2              26.94  ...        120.34359                   0   \n",
      "\n",
      "   high_decel_efforts  total_accel_decel  explosive_efforts  \\\n",
      "0                   2                 28                  9   \n",
      "1                   1                 14                  5   \n",
      "2                   3                112                 19   \n",
      "\n",
      "   high_metabolic_distance  equivalent_distance     avg_hr  max_hr  \\\n",
      "0                389.39999           1688.17285  141.94557     183   \n",
      "1                280.16000           1445.65173   84.90323      86   \n",
      "2               2010.90997           8245.02991  139.35739     183   \n",
      "\n",
      "   hr_exertion  \n",
      "0   2019.80005  \n",
      "1    903.19159  \n",
      "2   8421.97119  \n",
      "\n",
      "[3 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PILLAR 2: FEATURE ENGINEERING - GPS Physical Metrics\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PILLAR 2: Feature Engineering\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 2.1: Extract GPS Physical Features\n",
    "print(\"\\n[Step 2.1] Extracting GPS physical metrics...\")\n",
    "\n",
    "# Key GPS metrics based on the PDF methodology\n",
    "gps_features = {\n",
    "    # Volume metrics\n",
    "    'total_distance_(m)': 'total_distance',\n",
    "    'total_duration': 'total_duration',\n",
    "    \n",
    "    # Intensity metrics\n",
    "    'hs_distance_(m)': 'high_speed_distance',  # >19.8 km/h\n",
    "    'sprint': 'sprint_distance',  # >25.2 km/h\n",
    "    'distancia_>_20': 'distance_above_20',\n",
    "    'distancia_>_25': 'distance_above_25',\n",
    "    \n",
    "    # Player Load (accelerometer)\n",
    "    'total_player_load': 'total_player_load',\n",
    "    'player_load_per_minute': 'player_load_per_min',\n",
    "    'average_player_load': 'avg_player_load',\n",
    "    \n",
    "    # Explosiveness metrics\n",
    "    'acceleration_b3_efforts_(gen_2)': 'high_accel_efforts',\n",
    "    'deceleration_b3_efforts_(gen_2)': 'high_decel_efforts',\n",
    "    'accel_+_decel_efforts': 'total_accel_decel',\n",
    "    'explosive_efforts': 'explosive_efforts',\n",
    "    \n",
    "    # Metabolic power\n",
    "    'high_metabolic_load_distance_(m)': 'high_metabolic_distance',\n",
    "    'equivalent_distance_(m)': 'equivalent_distance',\n",
    "    \n",
    "    # Heart rate\n",
    "    'avg_heart_rate': 'avg_hr',\n",
    "    'maximum_heart_rate': 'max_hr',\n",
    "    'heart_rate_exertion': 'hr_exertion',\n",
    "}\n",
    "\n",
    "# Extract features from gps_all\n",
    "def extract_gps_features(gps_df, feature_map):\n",
    "    \"\"\"Extract and rename GPS features\"\"\"\n",
    "    available_features = {}\n",
    "    for old_col, new_col in feature_map.items():\n",
    "        if old_col in gps_df.columns:\n",
    "            available_features[old_col] = new_col\n",
    "    \n",
    "    # Select columns\n",
    "    id_cols = ['name', 'team_name', 'date', 'event_type']\n",
    "    feature_cols = list(available_features.keys())\n",
    "    \n",
    "    df = gps_df[id_cols + feature_cols].copy()\n",
    "    df.rename(columns={'name': 'player_name'}, inplace=True)\n",
    "    df.rename(columns=available_features, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "gps_features_df = extract_gps_features(gps_all, gps_features)\n",
    "\n",
    "print(f\"âœ“ Extracted {len(gps_features)} GPS features\")\n",
    "print(f\"âœ“ GPS features shape: {gps_features_df.shape}\")\n",
    "print(f\"\\nðŸ“Š Available GPS metrics:\")\n",
    "for col in gps_features_df.columns:\n",
    "    if col not in ['player_name', 'team_name', 'date', 'event_type']:\n",
    "        print(f\"  - {col}\")\n",
    "        \n",
    "# Show sample\n",
    "print(\"\\nðŸ“‹ Sample GPS features:\")\n",
    "print(gps_features_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d256d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 2.2] Extracting Wyscout technical metrics...\n",
      "âœ“ Extracted Wyscout features for 141558 outfield + 15323 GK records\n",
      "âœ“ Wyscout features shape: (156881, 31)\n",
      "\n",
      "ðŸ“Š Sample Wyscout metrics:\n",
      "          player_name team_name       date                        match  \\\n",
      "0        Roger IbaÃ±ez   AL AHLI 2021-03-18  Shakhtar Donetsk - Roma 1:2   \n",
      "1         Alaa  Hejji      NEOM 2023-04-24      Al Nassr - Al Wahda 0:1   \n",
      "2  Yousef Al Shammari  AL HAZEM 2021-12-20       Al Batin - Al Hazm 1:0   \n",
      "\n",
      "                  competition  goals  shots  shots_on_target    xg  assists  \\\n",
      "0  Europe. UEFA Europa League    0.0    0.0              0.0  0.00      0.0   \n",
      "1    Saudi Arabia. King's Cup    0.0    0.0              0.0  0.00      0.0   \n",
      "2    Saudi Arabia. King's Cup    0.0    1.0              0.0  0.03      0.0   \n",
      "\n",
      "   ...  defensive_duels_total  interceptions  aerial_duels_won  \\\n",
      "0  ...                   12.0            4.0               1.0   \n",
      "1  ...                    1.0            0.0               0.0   \n",
      "2  ...                    6.0            1.0               3.0   \n",
      "\n",
      "   aerial_duels_total  position_group  goals_conceded  shots_against  \\\n",
      "0                 1.0        Outfield             NaN            NaN   \n",
      "1                 0.0        Outfield             NaN            NaN   \n",
      "2                 5.0        Outfield             NaN            NaN   \n",
      "\n",
      "   shots_on_target_against  xcg  exits  \n",
      "0                      NaN  NaN    NaN  \n",
      "1                      NaN  NaN    NaN  \n",
      "2                      NaN  NaN    NaN  \n",
      "\n",
      "[3 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 2.2: Extract Wyscout Technical Features\n",
    "print(\"\\n[Step 2.2] Extracting Wyscout technical metrics...\")\n",
    "\n",
    "# Load Wyscout data\n",
    "wy_out = safe_read(DATA_DIR / \"wyscout_players_outfield.csv\")\n",
    "wy_gk = safe_read(DATA_DIR / \"wyscout_players_goalkeeper.csv\")\n",
    "\n",
    "wy_out = normalize_cols(wy_out)\n",
    "wy_gk = normalize_cols(wy_gk)\n",
    "\n",
    "# Parse dates\n",
    "wy_out['date'] = pd.to_datetime(wy_out['date'], errors='coerce')\n",
    "wy_gk['date'] = pd.to_datetime(wy_gk['date'], errors='coerce')\n",
    "\n",
    "# Position-specific KPIs from the PDF\n",
    "outfield_features = {\n",
    "    # Attack\n",
    "    'attack_buts': 'goals',\n",
    "    'attack_tirs_total': 'shots',\n",
    "    'attack_tirs_cadres': 'shots_on_target',\n",
    "    'attack_xg': 'xg',\n",
    "    'attack_passes_decisives': 'assists',\n",
    "    'attack_dribbles_reussis': 'successful_dribbles',\n",
    "    'attack_dribbles_total': 'total_dribbles',\n",
    "    'attack_duels_offensifs_gagnes': 'offensive_duels_won',\n",
    "    'attack_duels_offensifs_total': 'offensive_duels_total',\n",
    "    \n",
    "    # Passing\n",
    "    'general_passes_precises': 'accurate_passes',\n",
    "    'general_passes_total': 'total_passes',\n",
    "    'pass_passes_avant_precises': 'forward_passes_accurate',\n",
    "    'pass_passes_avant_total': 'forward_passes_total',\n",
    "    'pass_passes_tiers3_precises': 'final_third_passes_accurate',\n",
    "    'pass_passes_tiers3_total': 'final_third_passes_total',\n",
    "    \n",
    "    # Defense\n",
    "    'defense_duels_defensifs_gagnes': 'defensive_duels_won',\n",
    "    'defense_duels_defensifs_total': 'defensive_duels_total',\n",
    "    'defense_interceptions_total': 'interceptions',\n",
    "    'defense_duels_aeriens_gagnes': 'aerial_duels_won',\n",
    "    'defense_duels_aeriens_total': 'aerial_duels_total',\n",
    "}\n",
    "\n",
    "gk_features = {\n",
    "    'goalkeeper_buts_concedes': 'goals_conceded',\n",
    "    'goalkeeper_tirs_contre_total': 'shots_against',\n",
    "    'goalkeeper_tirs_contre_cadres': 'shots_on_target_against',\n",
    "    'goalkeeper_xcg': 'xcg',\n",
    "    'goalkeeper_sorties_total': 'exits',\n",
    "}\n",
    "\n",
    "# Extract outfield features\n",
    "def extract_wyscout_features(wy_df, feature_map, position_group):\n",
    "    \"\"\"Extract and rename Wyscout features\"\"\"\n",
    "    available_features = {}\n",
    "    for old_col, new_col in feature_map.items():\n",
    "        if old_col in wy_df.columns:\n",
    "            available_features[old_col] = new_col\n",
    "    \n",
    "    id_cols = ['player', 'team_name', 'date', 'match', 'competition']\n",
    "    feature_cols = list(available_features.keys())\n",
    "    \n",
    "    df = wy_df[id_cols + feature_cols].copy()\n",
    "    df.rename(columns={'player': 'player_name'}, inplace=True)\n",
    "    df.rename(columns=available_features, inplace=True)\n",
    "    df['position_group'] = position_group\n",
    "    \n",
    "    return df\n",
    "\n",
    "wy_out_features = extract_wyscout_features(wy_out, outfield_features, 'Outfield')\n",
    "wy_gk_features = extract_wyscout_features(wy_gk, gk_features, 'GK')\n",
    "\n",
    "# Combine\n",
    "wyscout_features_df = pd.concat([wy_out_features, wy_gk_features], ignore_index=True)\n",
    "\n",
    "print(f\"âœ“ Extracted Wyscout features for {wy_out_features.shape[0]} outfield + {wy_gk_features.shape[0]} GK records\")\n",
    "print(f\"âœ“ Wyscout features shape: {wyscout_features_df.shape}\")\n",
    "print(f\"\\nðŸ“Š Sample Wyscout metrics:\")\n",
    "print(wyscout_features_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d183462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 2.3] Merging all features...\n",
      "âœ“ Master dataset created: (12251, 53)\n",
      "âœ“ Columns: ['player_name', 'team_name', 'date', 'event_type', 'next_match_date', 'days_to_next_match', 'J_label', 'total_distance', 'total_duration', 'high_speed_distance', 'sprint_distance', 'distance_above_20', 'distance_above_25', 'total_player_load', 'player_load_per_min', 'avg_player_load', 'high_accel_efforts', 'high_decel_efforts', 'total_accel_decel', 'explosive_efforts', 'high_metabolic_distance', 'equivalent_distance', 'avg_hr', 'max_hr', 'hr_exertion', 'match', 'competition', 'goals', 'shots', 'shots_on_target', 'xg', 'assists', 'successful_dribbles', 'total_dribbles', 'offensive_duels_won', 'offensive_duels_total', 'accurate_passes', 'total_passes', 'forward_passes_accurate', 'forward_passes_total', 'final_third_passes_accurate', 'final_third_passes_total', 'defensive_duels_won', 'defensive_duels_total', 'interceptions', 'aerial_duels_won', 'aerial_duels_total', 'position_group', 'goals_conceded', 'shots_against', 'shots_on_target_against', 'xcg', 'exits']\n",
      "\n",
      "[Step 2.4] Calculating derived metrics...\n",
      "âœ“ Added derived percentage metrics\n",
      "\n",
      "ðŸ“Š Feature summary:\n",
      "  - Physical (GPS) features: ~18\n",
      "  - Technical (Wyscout) features: ~25\n",
      "  - Derived metrics: 4\n",
      "  - Total features: 57\n",
      "\n",
      "ðŸ“‹ Sample master dataset:\n",
      "             player_name team_name       date event_type next_match_date  \\\n",
      "0  Abdelmounaim Boutouil  AL HAZEM 2025-09-07   Training      2025-10-07   \n",
      "1  Abdelmounaim Boutouil  AL HAZEM 2025-09-07   Training      2025-10-07   \n",
      "2  Abdelmounaim Boutouil  AL HAZEM 2025-09-08   Training      2025-10-07   \n",
      "\n",
      "   days_to_next_match J_label  total_distance total_duration  \\\n",
      "0                30.0    J-30       905.91406       00:11:49   \n",
      "1                30.0    J-30      3873.34253       00:44:35   \n",
      "2                29.0    J-29      4916.03357       01:03:53   \n",
      "\n",
      "   high_speed_distance  ...  position_group  goals_conceded  shots_against  \\\n",
      "0                  0.0  ...             NaN             NaN            NaN   \n",
      "1                  0.0  ...             NaN             NaN            NaN   \n",
      "2                  0.0  ...             NaN             NaN            NaN   \n",
      "\n",
      "   shots_on_target_against  xcg  exits  passing_accuracy_pct  \\\n",
      "0                      NaN  NaN    NaN                   NaN   \n",
      "1                      NaN  NaN    NaN                   NaN   \n",
      "2                      NaN  NaN    NaN                   NaN   \n",
      "\n",
      "   dribble_success_pct  defensive_duels_won_pct  offensive_duels_won_pct  \n",
      "0                  NaN                      NaN                      NaN  \n",
      "1                  NaN                      NaN                      NaN  \n",
      "2                  NaN                      NaN                      NaN  \n",
      "\n",
      "[3 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 2.3: Merge Calendar + GPS + Wyscout\n",
    "print(\"\\n[Step 2.3] Merging all features...\")\n",
    "\n",
    "# Start with calendar (has J-days)\n",
    "master_df = calendar.copy()\n",
    "\n",
    "# Merge GPS features\n",
    "master_df = master_df.merge(\n",
    "    gps_features_df,\n",
    "    on=['player_name', 'team_name', 'date', 'event_type'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge Wyscout features (only for match days)\n",
    "master_df = master_df.merge(\n",
    "    wyscout_features_df,\n",
    "    on=['player_name', 'team_name', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Master dataset created: {master_df.shape}\")\n",
    "print(f\"âœ“ Columns: {list(master_df.columns)}\")\n",
    "\n",
    "# Calculate derived metrics\n",
    "print(\"\\n[Step 2.4] Calculating derived metrics...\")\n",
    "\n",
    "# Passing accuracy %\n",
    "if 'accurate_passes' in master_df.columns and 'total_passes' in master_df.columns:\n",
    "    master_df['passing_accuracy_pct'] = (master_df['accurate_passes'] / master_df['total_passes'] * 100).round(2)\n",
    "\n",
    "# Dribble success %\n",
    "if 'successful_dribbles' in master_df.columns and 'total_dribbles' in master_df.columns:\n",
    "    master_df['dribble_success_pct'] = (master_df['successful_dribbles'] / master_df['total_dribbles'] * 100).round(2)\n",
    "\n",
    "# Defensive duel win %\n",
    "if 'defensive_duels_won' in master_df.columns and 'defensive_duels_total' in master_df.columns:\n",
    "    master_df['defensive_duels_won_pct'] = (master_df['defensive_duels_won'] / master_df['defensive_duels_total'] * 100).round(2)\n",
    "\n",
    "# Offensive duel win %\n",
    "if 'offensive_duels_won' in master_df.columns and 'offensive_duels_total' in master_df.columns:\n",
    "    master_df['offensive_duels_won_pct'] = (master_df['offensive_duels_won'] / master_df['offensive_duels_total'] * 100).round(2)\n",
    "\n",
    "print(f\"âœ“ Added derived percentage metrics\")\n",
    "\n",
    "# Show summary\n",
    "print(\"\\nðŸ“Š Feature summary:\")\n",
    "print(f\"  - Physical (GPS) features: ~{len([c for c in master_df.columns if c in gps_features.values()])}\")\n",
    "print(f\"  - Technical (Wyscout) features: ~{len([c for c in master_df.columns if c in list(outfield_features.values()) + list(gk_features.values())])}\")\n",
    "print(f\"  - Derived metrics: 4\")\n",
    "print(f\"  - Total features: {master_df.shape[1]}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Sample master dataset:\")\n",
    "print(master_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adcbe7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PILLAR 2 VALIDATION\n",
      "============================================================\n",
      "\n",
      "âœ“ Match rows with GPS data: 4348 / 4348\n",
      "âœ“ Match rows with Wyscout data: 2536 / 4348\n",
      "âœ“ Training rows with GPS data: 7903 / 7903\n",
      "\n",
      "ðŸ“Š Missing values in key features:\n",
      "  total_distance           : 0.0% missing\n",
      "  high_speed_distance      : 0.0% missing\n",
      "  total_player_load        : 0.0% missing\n",
      "  goals                    : 79.3% missing\n",
      "  shots                    : 79.3% missing\n",
      "  accurate_passes          : 79.3% missing\n",
      "\n",
      "ðŸ“Š Data quality checks:\n",
      "  Total rows: 12251\n",
      "  Rows with J-label: 11360\n",
      "  Rows with GPS data: 12251\n",
      "  Date range: 2025-07-15 00:00:00 to 2025-11-06 00:00:00\n",
      "\n",
      "ðŸ’¾ Saved: data\\processed\\pillar2_master_features.csv\n",
      "\n",
      "âœ… PILLAR 2 COMPLETE - Ready for Pillar 3\n"
     ]
    }
   ],
   "source": [
    "# Step 2.5: Validate Pillar 2\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PILLAR 2 VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check 1: Feature coverage\n",
    "match_rows = master_df[master_df['event_type'] == 'Match']\n",
    "training_rows = master_df[master_df['event_type'] == 'Training']\n",
    "\n",
    "print(f\"\\nâœ“ Match rows with GPS data: {match_rows['total_distance'].notna().sum()} / {len(match_rows)}\")\n",
    "print(f\"âœ“ Match rows with Wyscout data: {match_rows['goals'].notna().sum() if 'goals' in match_rows.columns else 0} / {len(match_rows)}\")\n",
    "print(f\"âœ“ Training rows with GPS data: {training_rows['total_distance'].notna().sum()} / {len(training_rows)}\")\n",
    "\n",
    "# Check 2: Missing values analysis\n",
    "print(\"\\nðŸ“Š Missing values in key features:\")\n",
    "key_features = ['total_distance', 'high_speed_distance', 'total_player_load', 'goals', 'shots', 'accurate_passes']\n",
    "for feat in key_features:\n",
    "    if feat in master_df.columns:\n",
    "        missing_pct = (master_df[feat].isna().sum() / len(master_df) * 100)\n",
    "        print(f\"  {feat:25s}: {missing_pct:.1f}% missing\")\n",
    "\n",
    "# Check 3: Data quality\n",
    "print(\"\\nðŸ“Š Data quality checks:\")\n",
    "print(f\"  Total rows: {len(master_df)}\")\n",
    "print(f\"  Rows with J-label: {master_df['J_label'].notna().sum()}\")\n",
    "print(f\"  Rows with GPS data: {master_df['total_distance'].notna().sum()}\")\n",
    "print(f\"  Date range: {master_df['date'].min()} to {master_df['date'].max()}\")\n",
    "\n",
    "# Save Pillar 2 output\n",
    "master_df.to_csv(OUT_DIR / \"pillar2_master_features.csv\", index=False)\n",
    "print(f\"\\nðŸ’¾ Saved: {OUT_DIR / 'pillar2_master_features.csv'}\")\n",
    "\n",
    "print(\"\\nâœ… PILLAR 2 COMPLETE - Ready for Pillar 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538f486",
   "metadata": {},
   "source": [
    "---\n",
    "# PILLAR 3: AI-Powered Performance Prediction\n",
    "**Goal**: \n",
    "1. Generate historical Match Performance Rating (1-10)\n",
    "2. Train LSTM to predict next match rating based on training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b4c84a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PILLAR 3: AI-Powered Performance Prediction\n",
      "============================================================\n",
      "\n",
      "[Step 3.1] Generating Match Performance Ratings...\n",
      "âœ“ Match records: 4348\n",
      "\n",
      "ðŸ“Š Features for rating:\n",
      "  GPS features: 6\n",
      "  Wyscout features: 10\n",
      "âœ“ Matches with sufficient data for rating: 2536\n",
      "\n",
      "âœ“ Performance ratings generated!\n",
      "\n",
      "ðŸ“Š Rating statistics:\n",
      "count    2536.000000\n",
      "mean        2.218375\n",
      "std         0.952515\n",
      "min         1.000000\n",
      "25%         1.550000\n",
      "50%         1.940000\n",
      "75%         2.660000\n",
      "max        10.000000\n",
      "Name: match_performance_rating, dtype: float64\n",
      "\n",
      "ðŸ“‹ Sample match ratings:\n",
      "        player_name       date  match_performance_rating  goals  shots  \\\n",
      "2768  Aboubacar Bah 2025-08-28                      1.91    0.0    0.0   \n",
      "2769  Aboubacar Bah 2025-08-28                      2.02    0.0    0.0   \n",
      "2770  Aboubacar Bah 2025-08-28                      2.08    0.0    0.0   \n",
      "2771  Aboubacar Bah 2025-08-28                      3.29    0.0    1.0   \n",
      "2772  Aboubacar Bah 2025-08-28                      2.12    0.0    1.0   \n",
      "2773  Aboubacar Bah 2025-08-28                      1.78    0.0    0.0   \n",
      "2774  Aboubacar Bah 2025-08-28                      2.00    0.0    0.0   \n",
      "2775  Aboubacar Bah 2025-08-28                      2.65    0.0    0.0   \n",
      "2776  Aboubacar Bah 2025-08-28                      2.37    0.0    1.0   \n",
      "2777  Aboubacar Bah 2025-08-28                      3.37    0.0    0.0   \n",
      "\n",
      "      total_distance  \n",
      "2768      1468.47681  \n",
      "2769      1468.47681  \n",
      "2770      1468.47681  \n",
      "2771      1468.47681  \n",
      "2772      1468.47681  \n",
      "2773      1468.47681  \n",
      "2774      1468.47681  \n",
      "2775      1468.47681  \n",
      "2776      1468.47681  \n",
      "2777      9628.45740  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_4344\\1601541816.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rating_data['match_performance_rating'] = 1 + 9 * (weighted_scores - min_score) / (max_score - min_score)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_4344\\1601541816.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rating_data['match_performance_rating'] = rating_data['match_performance_rating'].round(2)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PILLAR 3.1: Generate Historical Match Performance Rating\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PILLAR 3: AI-Powered Performance Prediction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"\\n[Step 3.1] Generating Match Performance Ratings...\")\n",
    "\n",
    "# Filter match days only\n",
    "match_data = master_df[master_df['event_type'] == 'Match'].copy()\n",
    "\n",
    "print(f\"âœ“ Match records: {len(match_data)}\")\n",
    "\n",
    "# Define feature groups for rating calculation\n",
    "gps_rating_features = [\n",
    "    'total_distance', 'high_speed_distance', 'sprint_distance',\n",
    "    'total_player_load', 'high_accel_efforts', 'high_decel_efforts'\n",
    "]\n",
    "\n",
    "wyscout_rating_features = [\n",
    "    'goals', 'shots', 'xg', 'assists', 'successful_dribbles',\n",
    "    'accurate_passes', 'total_passes', 'offensive_duels_won',\n",
    "    'defensive_duels_won', 'interceptions'\n",
    "]\n",
    "\n",
    "# Filter available features\n",
    "available_gps = [f for f in gps_rating_features if f in match_data.columns]\n",
    "available_wyscout = [f for f in wyscout_rating_features if f in match_data.columns]\n",
    "\n",
    "print(f\"\\nðŸ“Š Features for rating:\")\n",
    "print(f\"  GPS features: {len(available_gps)}\")\n",
    "print(f\"  Wyscout features: {len(available_wyscout)}\")\n",
    "\n",
    "# Create rating dataset (only matches with sufficient data)\n",
    "rating_data = match_data.dropna(subset=available_gps + available_wyscout[:3])  # At least some key Wyscout features\n",
    "\n",
    "print(f\"âœ“ Matches with sufficient data for rating: {len(rating_data)}\")\n",
    "\n",
    "if len(rating_data) > 0:\n",
    "    # Combine features\n",
    "    all_features = available_gps + available_wyscout\n",
    "    X = rating_data[all_features].fillna(0)\n",
    "    \n",
    "    # Standardize (Z-scores)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply PCA to determine feature weights\n",
    "    pca = PCA(n_components=min(5, len(all_features)))\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Use first principal component as basis for weights\n",
    "    weights = np.abs(pca.components_[0])\n",
    "    weights = weights / weights.sum()  # Normalize\n",
    "    \n",
    "    # Calculate weighted score\n",
    "    weighted_scores = (X_scaled * weights).sum(axis=1)\n",
    "    \n",
    "    # Normalize to 1-10 scale\n",
    "    min_score, max_score = weighted_scores.min(), weighted_scores.max()\n",
    "    rating_data['match_performance_rating'] = 1 + 9 * (weighted_scores - min_score) / (max_score - min_score)\n",
    "    rating_data['match_performance_rating'] = rating_data['match_performance_rating'].round(2)\n",
    "    \n",
    "    print(f\"\\nâœ“ Performance ratings generated!\")\n",
    "    print(f\"\\nðŸ“Š Rating statistics:\")\n",
    "    print(rating_data['match_performance_rating'].describe())\n",
    "    \n",
    "    # Show examples\n",
    "    print(\"\\nðŸ“‹ Sample match ratings:\")\n",
    "    sample_cols = ['player_name', 'date', 'match_performance_rating', 'goals', 'shots', 'total_distance']\n",
    "    print(rating_data[sample_cols].head(10))\n",
    "else:\n",
    "    print(\"âš ï¸ Not enough data to generate ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ecbc107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 3.2] Preparing time-series sequences for LSTM...\n",
      "\n",
      "âœ“ Created 212317 sequences\n",
      "âœ“ Sequence shape: (212317, 10, 6)\n",
      "  - Samples: 212317\n",
      "  - Time steps: 10\n",
      "  - Features per step: 6\n",
      "âœ“ Target shape: (212317,)\n",
      "\n",
      "ðŸ“Š Target (rating) distribution:\n",
      "  Mean: 2.21\n",
      "  Std: 0.95\n",
      "  Min: 1.00\n",
      "  Max: 10.00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PILLAR 3.2: Prepare Time-Series Data for LSTM\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[Step 3.2] Preparing time-series sequences for LSTM...\")\n",
    "\n",
    "# Merge ratings back to master dataset\n",
    "master_df = master_df.merge(\n",
    "    rating_data[['player_name', 'date', 'match_performance_rating']],\n",
    "    on=['player_name', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# For each player, we need sequences of training days leading to a match\n",
    "def create_sequences(df, sequence_length=7):\n",
    "    \"\"\"\n",
    "    Create sequences of training data leading up to matches\n",
    "    \n",
    "    For each match, look back `sequence_length` days and collect:\n",
    "    - Daily GPS metrics (training load)\n",
    "    - Last match performance rating\n",
    "    - J-label context\n",
    "    \n",
    "    Target: Next match performance rating\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    metadata = []\n",
    "    \n",
    "    for player in df['player_name'].unique():\n",
    "        player_data = df[df['player_name'] == player].sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Find matches with ratings\n",
    "        match_indices = player_data[\n",
    "            (player_data['event_type'] == 'Match') & \n",
    "            (player_data['match_performance_rating'].notna())\n",
    "        ].index\n",
    "        \n",
    "        for match_idx in match_indices:\n",
    "            match_row = player_data.iloc[match_idx]\n",
    "            match_rating = match_row['match_performance_rating']\n",
    "            \n",
    "            # Look back for training data\n",
    "            lookback_start = max(0, match_idx - sequence_length)\n",
    "            sequence_data = player_data.iloc[lookback_start:match_idx]\n",
    "            \n",
    "            if len(sequence_data) > 0:\n",
    "                # Extract features for sequence\n",
    "                seq_features = []\n",
    "                for _, row in sequence_data.iterrows():\n",
    "                    day_features = [\n",
    "                        row.get('total_distance', 0) or 0,\n",
    "                        row.get('high_speed_distance', 0) or 0,\n",
    "                        row.get('total_player_load', 0) or 0,\n",
    "                        row.get('high_accel_efforts', 0) or 0,\n",
    "                        1 if row['event_type'] == 'Match' else 0,  # Is match day\n",
    "                        row.get('days_to_next_match', 0) or 0,\n",
    "                    ]\n",
    "                    seq_features.append(day_features)\n",
    "                \n",
    "                # Pad if needed\n",
    "                while len(seq_features) < sequence_length:\n",
    "                    seq_features.insert(0, [0] * len(seq_features[0]))\n",
    "                \n",
    "                sequences.append(seq_features[:sequence_length])\n",
    "                targets.append(match_rating)\n",
    "                metadata.append({\n",
    "                    'player': player,\n",
    "                    'date': match_row['date'],\n",
    "                    'team': match_row.get('team_name', ''),\n",
    "                })\n",
    "    \n",
    "    return np.array(sequences), np.array(targets), metadata\n",
    "\n",
    "# Create sequences\n",
    "SEQ_LENGTH = 10  # Look back 10 days before each match\n",
    "X_seq, y_seq, seq_meta = create_sequences(master_df, sequence_length=SEQ_LENGTH)\n",
    "\n",
    "print(f\"\\nâœ“ Created {len(X_seq)} sequences\")\n",
    "print(f\"âœ“ Sequence shape: {X_seq.shape}\")\n",
    "print(f\"  - Samples: {X_seq.shape[0]}\")\n",
    "print(f\"  - Time steps: {X_seq.shape[1]}\")\n",
    "print(f\"  - Features per step: {X_seq.shape[2]}\")\n",
    "print(f\"âœ“ Target shape: {y_seq.shape}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Target (rating) distribution:\")\n",
    "print(f\"  Mean: {y_seq.mean():.2f}\")\n",
    "print(f\"  Std: {y_seq.std():.2f}\")\n",
    "print(f\"  Min: {y_seq.min():.2f}\")\n",
    "print(f\"  Max: {y_seq.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "274dc95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 3.3] Building LSTM model...\n",
      "âœ“ Training set: 169853 samples\n",
      "âœ“ Test set: 42464 samples\n",
      "\n",
      "âœ“ Features normalized\n",
      "\n",
      "[Building LSTM Architecture...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LSTM model built!\n",
      "\n",
      "ðŸ“‹ Model architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,176</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m18,176\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             â”‚           \u001b[38;5;34m528\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m17\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,137</span> (121.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,137\u001b[0m (121.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,137</span> (121.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,137\u001b[0m (121.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training LSTM...]\n",
      "âœ“ Training complete!\n",
      "\n",
      "ðŸ“Š Test Performance:\n",
      "  Loss (MSE): 0.7868\n",
      "  MAE: 0.6668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Sample predictions:\n",
      "  Actual: 2.04 | Predicted: 2.12\n",
      "  Actual: 2.73 | Predicted: 2.16\n",
      "  Actual: 1.31 | Predicted: 2.16\n",
      "  Actual: 1.53 | Predicted: 2.31\n",
      "  Actual: 1.24 | Predicted: 2.14\n",
      "\n",
      "ðŸ’¾ Model saved: data\\processed\\lstm_performance_predictor.h5\n",
      "\n",
      "âœ… PILLAR 3 COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PILLAR 3.3: Build and Train LSTM Model\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[Step 3.3] Building LSTM model...\")\n",
    "\n",
    "# Check if we have enough data\n",
    "if len(X_seq) < 10:\n",
    "    print(\"âš ï¸ Not enough sequences for training (need at least 10)\")\n",
    "    print(\"   This is likely due to limited match data with ratings.\")\n",
    "    print(\"   The pipeline is ready - just need more data!\")\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_seq, y_seq, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"âœ“ Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Normalize features\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    # Reshape for scaling\n",
    "    n_samples, n_steps, n_features = X_train.shape\n",
    "    X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "    X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "    \n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train_reshaped).reshape(n_samples, n_steps, n_features)\n",
    "    X_test_scaled = scaler_X.transform(X_test_reshaped).reshape(X_test.shape[0], n_steps, n_features)\n",
    "    \n",
    "    print(\"\\nâœ“ Features normalized\")\n",
    "    \n",
    "    # Build LSTM model\n",
    "    print(\"\\n[Building LSTM Architecture...]\")\n",
    "    \n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras import layers\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            layers.LSTM(64, activation='relu', return_sequences=True, input_shape=(SEQ_LENGTH, n_features)),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.LSTM(32, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(16, activation='relu'),\n",
    "            layers.Dense(1)  # Output: predicted rating\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        print(\"âœ“ LSTM model built!\")\n",
    "        print(\"\\nðŸ“‹ Model architecture:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Train model\n",
    "        print(\"\\n[Training LSTM...]\")\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=50,\n",
    "            batch_size=16,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        print(\"âœ“ Training complete!\")\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "        print(f\"\\nðŸ“Š Test Performance:\")\n",
    "        print(f\"  Loss (MSE): {test_loss:.4f}\")\n",
    "        print(f\"  MAE: {test_mae:.4f}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_scaled, verbose=0).flatten()\n",
    "        \n",
    "        print(\"\\nðŸ“‹ Sample predictions:\")\n",
    "        for i in range(min(5, len(y_test))):\n",
    "            print(f\"  Actual: {y_test[i]:.2f} | Predicted: {y_pred[i]:.2f}\")\n",
    "        \n",
    "        # Save model\n",
    "        model.save(OUT_DIR / \"lstm_performance_predictor.h5\")\n",
    "        print(f\"\\nðŸ’¾ Model saved: {OUT_DIR / 'lstm_performance_predictor.h5'}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ TensorFlow not installed. Install with: pip install tensorflow\")\n",
    "        print(\"   The data preparation is complete - just need TensorFlow to train the model!\")\n",
    "    \n",
    "print(\"\\nâœ… PILLAR 3 COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3537937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ¯ CYCLE-TO-PEAK PIPELINE SUMMARY\n",
      "============================================================\n",
      "\n",
      "âœ… PILLAR 1: Training Cycle Calculation\n",
      "  â€¢ Created calendar with J-day labels\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calendar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… PILLAR 1: Training Cycle Calculation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  â€¢ Created calendar with J-day labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  â€¢ Total records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mcalendar\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  â€¢ Players tracked: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalendar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayer_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  â€¢ Date range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalendar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mdate()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalendar[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mdate()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calendar' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINAL VALIDATION & SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ CYCLE-TO-PEAK PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nâœ… PILLAR 1: Training Cycle Calculation\")\n",
    "print(f\"  â€¢ Created calendar with J-day labels\")\n",
    "print(f\"  â€¢ Total records: {len(calendar)}\")\n",
    "print(f\"  â€¢ Players tracked: {calendar['player_name'].nunique()}\")\n",
    "print(f\"  â€¢ Date range: {calendar['date'].min().date()} to {calendar['date'].max().date()}\")\n",
    "\n",
    "print(\"\\nâœ… PILLAR 2: Feature Engineering\")\n",
    "print(f\"  â€¢ Master dataset: {master_df.shape[0]} rows Ã— {master_df.shape[1]} columns\")\n",
    "print(f\"  â€¢ GPS physical metrics: {len(available_gps)}\")\n",
    "print(f\"  â€¢ Wyscout technical metrics: {len(available_wyscout)}\")\n",
    "print(f\"  â€¢ Match records: {len(master_df[master_df['event_type'] == 'Match'])}\")\n",
    "print(f\"  â€¢ Training records: {len(master_df[master_df['event_type'] == 'Training'])}\")\n",
    "\n",
    "print(\"\\nâœ… PILLAR 3: Performance Prediction\")\n",
    "print(f\"  â€¢ Match ratings generated: {rating_data['match_performance_rating'].notna().sum()}\")\n",
    "print(f\"  â€¢ LSTM sequences created: {len(X_seq)}\")\n",
    "print(f\"  â€¢ Sequence length: {SEQ_LENGTH} days\")\n",
    "if len(X_seq) >= 10:\n",
    "    print(f\"  â€¢ Model trained: YES\")\n",
    "    print(f\"  â€¢ Test MAE: {test_mae:.4f}\")\n",
    "else:\n",
    "    print(f\"  â€¢ Model trained: NO (need more match data)\")\n",
    "\n",
    "print(\"\\nðŸ“ Output Files:\")\n",
    "print(f\"  â€¢ {OUT_DIR / 'pillar1_calendar_j_days.csv'}\")\n",
    "print(f\"  â€¢ {OUT_DIR / 'pillar2_master_features.csv'}\")\n",
    "if len(X_seq) >= 10:\n",
    "    print(f\"  â€¢ {OUT_DIR / 'lstm_performance_predictor.h5'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“ Next Steps:\")\n",
    "print(\"  1. Review the generated files in data/processed/\")\n",
    "print(\"  2. Validate J-day calculations for specific players\")\n",
    "print(\"  3. Check feature distributions and missing values\")\n",
    "print(\"  4. If model trained, use it to predict upcoming match readiness\")\n",
    "print(\"  5. Integrate predictions into web dashboard\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Usage Example:\")\n",
    "print(\"   - Load master_features.csv\")\n",
    "print(\"   - Filter for upcoming match (next J-Day)\")\n",
    "print(\"   - Extract last 10 days of training data\")\n",
    "print(\"   - Feed to LSTM model â†’ Get Match Readiness Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fb9256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras2onnx\n",
      "  Downloading keras2onnx-1.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from keras2onnx) (2.1.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from keras2onnx) (5.29.4)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from keras2onnx) (2.32.3)\n",
      "Collecting onnx (from keras2onnx)\n",
      "  Downloading onnx-1.19.1-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting onnxconverter-common>=1.7.0 (from keras2onnx)\n",
      "  Downloading onnxconverter_common-1.16.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fire (from keras2onnx)\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from onnxconverter-common>=1.7.0->keras2onnx) (24.2)\n",
      "Requirement already satisfied: termcolor in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from fire->keras2onnx) (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from onnx->keras2onnx) (4.13.2)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from onnx->keras2onnx) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests->keras2onnx) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests->keras2onnx) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests->keras2onnx) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\envs\\tf_env\\lib\\site-packages (from requests->keras2onnx) (2025.1.31)\n",
      "Downloading keras2onnx-1.7.0-py3-none-any.whl (96 kB)\n",
      "Downloading onnxconverter_common-1.16.0-py2.py3-none-any.whl (89 kB)\n",
      "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Downloading onnx-1.19.1-cp310-cp310-win_amd64.whl (16.5 MB)\n",
      "   ---------------------------------------- 0.0/16.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/16.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/16.5 MB 1.5 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 1.0/16.5 MB 2.0 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.3/16.5 MB 1.8 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.8/16.5 MB 1.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 2.1/16.5 MB 1.7 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.1/16.5 MB 1.7 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.6/16.5 MB 1.6 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.9/16.5 MB 1.6 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.1/16.5 MB 1.6 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 3.7/16.5 MB 1.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.9/16.5 MB 1.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 4.2/16.5 MB 1.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 4.5/16.5 MB 1.5 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 4.7/16.5 MB 1.5 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 5.0/16.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 5.0/16.5 MB 1.5 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 5.5/16.5 MB 1.5 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 6.3/16.5 MB 1.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 6.6/16.5 MB 1.6 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 6.8/16.5 MB 1.6 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 6.8/16.5 MB 1.6 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 7.1/16.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 7.1/16.5 MB 1.5 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 7.3/16.5 MB 1.4 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 7.3/16.5 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 7.6/16.5 MB 1.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 7.9/16.5 MB 1.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 8.1/16.5 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 8.7/16.5 MB 1.4 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 8.9/16.5 MB 1.4 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 9.4/16.5 MB 1.4 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 9.7/16.5 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 10.0/16.5 MB 1.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 10.5/16.5 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 10.7/16.5 MB 1.4 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 11.3/16.5 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.8/16.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 12.1/16.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.6/16.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 13.1/16.5 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 13.4/16.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 14.2/16.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.7/16.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 15.2/16.5 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.0/16.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.5/16.5 MB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: fire, onnx, onnxconverter-common, keras2onnx\n",
      "Successfully installed fire-0.7.1 keras2onnx-1.7.0 onnx-1.19.1 onnxconverter-common-1.16.0\n"
     ]
    }
   ],
   "source": [
    "! pip install keras2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fd3ee",
   "metadata": {},
   "source": [
    "# ðŸ“Š Visualisation des RÃ©sultats du ModÃ¨le\n",
    "\n",
    "Cette section prÃ©sente des visualisations complÃ¨tes pour Ã©valuer correctement les performances du modÃ¨le LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57c35227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Colonne 'match_performance_rating' absente. Recalcul en cours...\n",
      "âœ… Ratings recalculÃ©s pour 2536 matchs.\n",
      "âœ… Ratings recalculÃ©s pour 2536 matchs.\n",
      "âœ… SÃ©quences rechargÃ©es: 212317 | Forme: (212317, 10, 6)\n",
      "   â€¢ Cible: (212317,)\n",
      "âœ… SÃ©quences rechargÃ©es: 212317 | Forme: (212317, 10, 6)\n",
      "   â€¢ Cible: (212317,)\n",
      "âœ… Jeux recrÃ©Ã©s: 169853 entraÃ®nement | 42464 test\n",
      "   â€¢ Nombre de features: 6\n",
      "\n",
      "Les variables clÃ©s (calendar, master_df, rating_data, X_seq, y_seq, X_train_scaled, X_test_scaled, etc.) sont prÃªtes pour les visualisations.\n",
      "âœ… Jeux recrÃ©Ã©s: 169853 entraÃ®nement | 42464 test\n",
      "   â€¢ Nombre de features: 6\n",
      "\n",
      "Les variables clÃ©s (calendar, master_df, rating_data, X_seq, y_seq, X_train_scaled, X_test_scaled, etc.) sont prÃªtes pour les visualisations.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RECHARGEMENT DES DONNÃ‰ES POUR LES VISUALISATIONS\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Dossiers de travail\n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "calendar_path = OUT_DIR / \"pillar1_calendar_j_days.csv\"\n",
    "master_path = OUT_DIR / \"pillar2_master_features.csv\"\n",
    "model_path = OUT_DIR / \"lstm_performance_predictor.h5\"\n",
    "\n",
    "if not calendar_path.exists() or not master_path.exists():\n",
    "    print(\"âŒ Fichiers prÃ©traitÃ©s introuvables dans data/processed. ExÃ©cutez d'abord le pipeline complet.\")\n",
    "else:\n",
    "    # Charger les sorties prÃ©-calculÃ©es\n",
    "    calendar = pd.read_csv(calendar_path)\n",
    "    master_df = pd.read_csv(master_path)\n",
    "\n",
    "    # Harmoniser les dates\n",
    "    for df in (calendar, master_df):\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "    rating_col = 'match_performance_rating'\n",
    "\n",
    "    def _recompute_match_ratings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Recalcule la note de performance si elle est absente dans master_df.\"\"\"\n",
    "        print(\"â„¹ï¸ Colonne 'match_performance_rating' absente. Recalcul en cours...\")\n",
    "\n",
    "        match_data = df[df['event_type'] == 'Match'].copy()\n",
    "        if match_data.empty:\n",
    "            print(\"âš ï¸ Aucun match disponible pour calculer les ratings.\")\n",
    "            return pd.DataFrame(columns=['player_name', 'team_name', 'date', rating_col])\n",
    "\n",
    "        gps_rating_features = [\n",
    "            'total_distance', 'high_speed_distance', 'sprint_distance',\n",
    "            'total_player_load', 'high_accel_efforts', 'high_decel_efforts'\n",
    "        ]\n",
    "        wyscout_rating_features = [\n",
    "            'goals', 'shots', 'xg', 'assists', 'successful_dribbles',\n",
    "            'accurate_passes', 'total_passes', 'offensive_duels_won',\n",
    "            'defensive_duels_won', 'interceptions'\n",
    "        ]\n",
    "\n",
    "        available_gps = [f for f in gps_rating_features if f in match_data.columns]\n",
    "        available_wyscout = [f for f in wyscout_rating_features if f in match_data.columns]\n",
    "\n",
    "        if not available_gps and not available_wyscout:\n",
    "            print(\"âš ï¸ Impossible de recalculer les ratings: aucune feature pertinente disponible.\")\n",
    "            return pd.DataFrame(columns=['player_name', 'team_name', 'date', rating_col])\n",
    "\n",
    "        required_subset = available_gps + available_wyscout[:min(3, len(available_wyscout))]\n",
    "        rating_candidates = match_data.dropna(subset=required_subset) if required_subset else match_data\n",
    "\n",
    "        if rating_candidates.empty:\n",
    "            print(\"âš ï¸ Aucune ligne ne possÃ¨de toutes les features nÃ©cessaires pour recalculer les ratings.\")\n",
    "            return pd.DataFrame(columns=['player_name', 'team_name', 'date', rating_col])\n",
    "\n",
    "        all_features = available_gps + available_wyscout\n",
    "        if not all_features:\n",
    "            print(\"âš ï¸ Aucun ensemble complet de features pour le calcul des ratings.\")\n",
    "            return pd.DataFrame(columns=['player_name', 'team_name', 'date', rating_col])\n",
    "\n",
    "        X = rating_candidates[all_features].fillna(0.0)\n",
    "\n",
    "        try:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            from sklearn.decomposition import PCA\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "            n_components = min(5, X_scaled.shape[1], X_scaled.shape[0])\n",
    "            if n_components < 1:\n",
    "                raise ValueError(\"Nombre de composantes PCA insuffisant.\")\n",
    "\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca.fit(X_scaled)\n",
    "            weights = np.abs(pca.components_[0])\n",
    "            if weights.sum() == 0:\n",
    "                weights = np.ones_like(weights) / len(weights)\n",
    "            else:\n",
    "                weights = weights / weights.sum()\n",
    "\n",
    "            weighted_scores = (X_scaled * weights).sum(axis=1)\n",
    "        except Exception as exc:\n",
    "            print(f\"âš ï¸ PCA indisponible ({exc}). Utilisation d'une moyenne pondÃ©rÃ©e simple.\")\n",
    "            X_min = X.min(axis=0)\n",
    "            X_max = X.max(axis=0)\n",
    "            denom = (X_max - X_min).replace(0, 1)\n",
    "            X_normalized = (X - X_min) / denom\n",
    "            weighted_scores = X_normalized.mean(axis=1)\n",
    "\n",
    "        min_score = float(weighted_scores.min())\n",
    "        max_score = float(weighted_scores.max())\n",
    "        if max_score == min_score:\n",
    "            normalized_scores = np.full_like(weighted_scores, 5.0, dtype=float)\n",
    "        else:\n",
    "            normalized_scores = 1 + 9 * (weighted_scores - min_score) / (max_score - min_score)\n",
    "\n",
    "        ratings_df = rating_candidates[['player_name', 'team_name', 'date']].copy()\n",
    "        ratings_df[rating_col] = np.round(normalized_scores, 2)\n",
    "        print(f\"âœ… Ratings recalculÃ©s pour {len(ratings_df)} matchs.\")\n",
    "        return ratings_df\n",
    "\n",
    "    if rating_col not in master_df.columns:\n",
    "        ratings_recomputed = _recompute_match_ratings(master_df)\n",
    "        if not ratings_recomputed.empty:\n",
    "            master_df = master_df.merge(\n",
    "                ratings_recomputed,\n",
    "                on=['player_name', 'team_name', 'date'],\n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            master_df[rating_col] = np.nan\n",
    "\n",
    "    # Reconstituer les notations de match (si disponibles)\n",
    "    if rating_col in master_df.columns:\n",
    "        rating_data = (\n",
    "            master_df[\n",
    "                (master_df['event_type'] == 'Match') &\n",
    "                (master_df[rating_col].notna())\n",
    "            ][['player_name', 'team_name', 'date', rating_col]]\n",
    "            .drop_duplicates()\n",
    "            .sort_values(['player_name', 'date'])\n",
    "        )\n",
    "    else:\n",
    "        rating_data = pd.DataFrame(columns=['player_name', 'team_name', 'date', rating_col])\n",
    "\n",
    "    # Ordonnancer l'ensemble principal\n",
    "    master_df = master_df.sort_values(['player_name', 'date']).reset_index(drop=True)\n",
    "\n",
    "    # ParamÃ¨tres de sÃ©quences (identiques Ã  l'entraÃ®nement)\n",
    "    selected_features = [\n",
    "        'total_distance',\n",
    "        'high_speed_distance',\n",
    "        'total_player_load',\n",
    "        'high_accel_efforts',\n",
    "        'is_match_day',\n",
    "        'days_to_next_match',\n",
    "    ]\n",
    "    n_features = len(selected_features)\n",
    "    SEQ_LENGTH = 10\n",
    "\n",
    "    def _build_day_vector(row: pd.Series) -> list[float]:\n",
    "        \"\"\"Transforme une ligne en vecteur de features pour la sÃ©quence temporelle.\"\"\"\n",
    "        return [\n",
    "            float(row.get('total_distance', 0) or 0),\n",
    "            float(row.get('high_speed_distance', 0) or 0),\n",
    "            float(row.get('total_player_load', 0) or 0),\n",
    "            float(row.get('high_accel_efforts', 0) or 0),\n",
    "            1.0 if row.get('event_type') == 'Match' else 0.0,\n",
    "            float(row.get('days_to_next_match', 0) or 0),\n",
    "        ]\n",
    "\n",
    "    def recreate_sequences(df: pd.DataFrame, sequence_length: int = SEQ_LENGTH):\n",
    "        \"\"\"RecrÃ©e les sÃ©quences LSTM Ã  partir du master_df sauvegardÃ©.\"\"\"\n",
    "        sequences, targets, metadata = [], [], []\n",
    "\n",
    "        if rating_col not in df.columns:\n",
    "            return np.array([]), np.array([]), []\n",
    "\n",
    "        for player, player_data in df.groupby('player_name'):\n",
    "            player_data = player_data.sort_values('date').reset_index(drop=True)\n",
    "            match_indices = player_data[\n",
    "                (player_data['event_type'] == 'Match') &\n",
    "                (player_data[rating_col].notna())\n",
    "            ].index\n",
    "\n",
    "            for match_idx in match_indices:\n",
    "                target_row = player_data.iloc[match_idx]\n",
    "                lookback_start = max(0, match_idx - sequence_length)\n",
    "                history_window = player_data.iloc[lookback_start:match_idx]\n",
    "\n",
    "                if history_window.empty:\n",
    "                    continue\n",
    "\n",
    "                seq_rows = [_build_day_vector(row) for _, row in history_window.iterrows()]\n",
    "\n",
    "                while len(seq_rows) < sequence_length:\n",
    "                    seq_rows.insert(0, [0.0] * n_features)\n",
    "\n",
    "                sequences.append(seq_rows[:sequence_length])\n",
    "                targets.append(float(target_row[rating_col]))\n",
    "                metadata.append({\n",
    "                    'player': player,\n",
    "                    'team': target_row.get('team_name'),\n",
    "                    'date': target_row['date'],\n",
    "                })\n",
    "\n",
    "        return np.array(sequences, dtype=float), np.array(targets, dtype=float), metadata\n",
    "\n",
    "    X_seq, y_seq, seq_meta = recreate_sequences(master_df, sequence_length=SEQ_LENGTH)\n",
    "\n",
    "    if len(X_seq) == 0:\n",
    "        print(\"âš ï¸ Aucune sÃ©quence gÃ©nÃ©rÃ©e. VÃ©rifiez que 'match_performance_rating' est disponible ou recalculÃ©e correctement.\")\n",
    "    else:\n",
    "        print(f\"âœ… SÃ©quences rechargÃ©es: {len(X_seq)} | Forme: {X_seq.shape}\")\n",
    "        print(f\"   â€¢ Cible: {y_seq.shape}\")\n",
    "\n",
    "        if len(X_seq) < 10:\n",
    "            print(\"âš ï¸ Peu de sÃ©quences disponibles (<10). Les visualisations pourront Ãªtre limitÃ©es.\")\n",
    "        else:\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_seq, y_seq, test_size=0.2, random_state=42\n",
    "            )\n",
    "\n",
    "            scaler_X = MinMaxScaler()\n",
    "            n_steps = X_train.shape[1]\n",
    "\n",
    "            X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, n_features)).reshape(-1, n_steps, n_features)\n",
    "            X_test_scaled = scaler_X.transform(X_test.reshape(-1, n_features)).reshape(-1, n_steps, n_features)\n",
    "\n",
    "            print(f\"âœ… Jeux recrÃ©Ã©s: {X_train.shape[0]} entraÃ®nement | {X_test.shape[0]} test\")\n",
    "            print(f\"   â€¢ Nombre de features: {n_features}\")\n",
    "\n",
    "    print(\"\\nLes variables clÃ©s (calendar, master_df, rating_data, X_seq, y_seq, X_train_scaled, X_test_scaled, etc.) sont prÃªtes pour les visualisations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e319d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Chargement du modÃ¨le depuis: data\\processed\\lstm_performance_predictor.h5\n",
      "âœ… ModÃ¨le chargÃ© avec succÃ¨s!\n",
      "\n",
      "ðŸ“‹ Architecture du modÃ¨le:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,176</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m18,176\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             â”‚           \u001b[38;5;34m528\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m17\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,139</span> (121.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,139\u001b[0m (121.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,137</span> (121.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,137\u001b[0m (121.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ RÃ©gÃ©nÃ©ration des prÃ©dictions avec le modÃ¨le chargÃ©...\n",
      "âœ… PrÃ©dictions gÃ©nÃ©rÃ©es: 42464 Ã©chantillons\n",
      "\n",
      "ðŸ“Š MÃ©triques calculÃ©es:\n",
      "   â€¢ MSE (Keras): 0.7868\n",
      "   â€¢ MAE (Keras): 0.6668\n",
      "   â€¢ MAE (sklearn): 0.6668\n",
      "   â€¢ RMSE: 0.8870\n",
      "   â€¢ RÂ²: 0.1321\n",
      "   â€¢ MAPE: 31.81%\n",
      "\n",
      "ðŸ“Š AperÃ§u des prÃ©dictions:\n",
      "  Match 1: RÃ©el=2.04 | PrÃ©dit=2.12 | Erreur=0.08\n",
      "  Match 2: RÃ©el=2.73 | PrÃ©dit=2.16 | Erreur=0.57\n",
      "  Match 3: RÃ©el=1.31 | PrÃ©dit=2.16 | Erreur=0.85\n",
      "  Match 4: RÃ©el=1.53 | PrÃ©dit=2.31 | Erreur=0.78\n",
      "  Match 5: RÃ©el=1.24 | PrÃ©dit=2.14 | Erreur=0.90\n",
      "âœ… PrÃ©dictions gÃ©nÃ©rÃ©es: 42464 Ã©chantillons\n",
      "\n",
      "ðŸ“Š MÃ©triques calculÃ©es:\n",
      "   â€¢ MSE (Keras): 0.7868\n",
      "   â€¢ MAE (Keras): 0.6668\n",
      "   â€¢ MAE (sklearn): 0.6668\n",
      "   â€¢ RMSE: 0.8870\n",
      "   â€¢ RÂ²: 0.1321\n",
      "   â€¢ MAPE: 31.81%\n",
      "\n",
      "ðŸ“Š AperÃ§u des prÃ©dictions:\n",
      "  Match 1: RÃ©el=2.04 | PrÃ©dit=2.12 | Erreur=0.08\n",
      "  Match 2: RÃ©el=2.73 | PrÃ©dit=2.16 | Erreur=0.57\n",
      "  Match 3: RÃ©el=1.31 | PrÃ©dit=2.16 | Erreur=0.85\n",
      "  Match 4: RÃ©el=1.53 | PrÃ©dit=2.31 | Erreur=0.78\n",
      "  Match 5: RÃ©el=1.24 | PrÃ©dit=2.14 | Erreur=0.90\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHARGEMENT DU MODÃˆLE SAUVEGARDÃ‰\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# VÃ©rifier si le modÃ¨le existe dans data/processed\n",
    "model_path = OUT_DIR / \"lstm_performance_predictor.h5\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"ðŸ“‚ Chargement du modÃ¨le depuis: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Import TensorFlow et Keras\n",
    "        import tensorflow as tf\n",
    "        from tensorflow import keras\n",
    "        \n",
    "        # Gestion des pertes/metrics sÃ©rialisÃ©s en texte\n",
    "        custom_objects = {\n",
    "            'mse': keras.losses.MeanSquaredError(),\n",
    "            'mae': keras.metrics.MeanAbsoluteError()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Charger le modÃ¨le (compile=True) avec custom_objects pour Ã©viter l'erreur \"Could not locate function\"\n",
    "            model = keras.models.load_model(str(model_path), custom_objects=custom_objects)\n",
    "        except TypeError as te:\n",
    "            if \"Could not locate function\" in str(te):\n",
    "                print(\"âš ï¸ Impossible de localiser certaines fonctions sÃ©rialisÃ©es (mse/mae). Chargement avec compile=False...\")\n",
    "                model = keras.models.load_model(str(model_path), custom_objects=custom_objects, compile=False)\n",
    "                model.compile(optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.MeanAbsoluteError()])\n",
    "                print(\"âœ… ModÃ¨le rechargÃ© et recompilÃ© avec succÃ¨s!\")\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        print(\"âœ… ModÃ¨le chargÃ© avec succÃ¨s!\")\n",
    "        print(\"\\nðŸ“‹ Architecture du modÃ¨le:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # VÃ©rifier que les variables nÃ©cessaires existent pour les visualisations\n",
    "        if 'X_test_scaled' not in globals() or 'y_test' not in globals():\n",
    "            print(\"\\nâš ï¸ Note: Les donnÃ©es de test (X_test_scaled, y_test) ne sont pas encore chargÃ©es.\")\n",
    "            print(\"   Vous devez exÃ©cuter les cellules prÃ©cÃ©dentes pour gÃ©nÃ©rer ces variables.\")\n",
    "        else:\n",
    "            # Ã‰valuer le modÃ¨le pour retrouver les mÃ©triques de test\n",
    "            test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "            \n",
    "            # RÃ©gÃ©nÃ©rer les prÃ©dictions avec le modÃ¨le chargÃ©\n",
    "            print(\"\\nðŸ”„ RÃ©gÃ©nÃ©ration des prÃ©dictions avec le modÃ¨le chargÃ©...\")\n",
    "            y_pred = model.predict(X_test_scaled, verbose=0).flatten()\n",
    "            residuals = y_test - y_pred\n",
    "            abs_errors = np.abs(residuals)\n",
    "            \n",
    "            # Calculer les mÃ©triques complÃ©mentaires\n",
    "            from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "            \n",
    "            # Calculer les quartiles pour les visualisations\n",
    "            quartiles = np.percentile(abs_errors, [25, 50, 75])\n",
    "            q1_count = np.sum(abs_errors <= quartiles[0])\n",
    "            q2_count = np.sum((abs_errors > quartiles[0]) & (abs_errors <= quartiles[1]))\n",
    "            q3_count = np.sum((abs_errors > quartiles[1]) & (abs_errors <= quartiles[2]))\n",
    "            q4_count = np.sum(abs_errors > quartiles[2])\n",
    "            quartile_counts = [q1_count, q2_count, q3_count, q4_count]\n",
    "            \n",
    "            # Calculer les prÃ©cisions par seuil\n",
    "            accuracy_within_05 = np.sum(abs_errors < 0.5) / len(abs_errors) * 100\n",
    "            accuracy_within_1 = np.sum(abs_errors < 1.0) / len(abs_errors) * 100\n",
    "            \n",
    "            print(f\"âœ… PrÃ©dictions gÃ©nÃ©rÃ©es: {len(y_pred)} Ã©chantillons\")\n",
    "            print(\"\\nðŸ“Š MÃ©triques calculÃ©es:\")\n",
    "            print(f\"   â€¢ MSE (Keras): {test_loss:.4f}\")\n",
    "            print(f\"   â€¢ MAE (Keras): {test_mae:.4f}\")\n",
    "            print(f\"   â€¢ MAE (sklearn): {mae:.4f}\")\n",
    "            print(f\"   â€¢ RMSE: {rmse:.4f}\")\n",
    "            print(f\"   â€¢ RÂ²: {r2:.4f}\")\n",
    "            print(f\"   â€¢ MAPE: {mape:.2f}%\")\n",
    "            print(\"\\nðŸ“Š AperÃ§u des prÃ©dictions:\")\n",
    "            for i in range(min(5, len(y_test))):\n",
    "                print(f\"  Match {i+1}: RÃ©el={y_test[i]:.2f} | PrÃ©dit={y_pred[i]:.2f} | Erreur={abs_errors[i]:.2f}\")\n",
    "                \n",
    "    except ImportError as ie:\n",
    "        print(f\"âŒ Erreur d'import: {ie}\")\n",
    "        print(\"   Installez TensorFlow avec: pip install tensorflow\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur lors du chargement du modÃ¨le: {e}\")\n",
    "        print(f\"   Type d'erreur: {type(e).__name__}\")\n",
    "else:\n",
    "    print(f\"âŒ Le fichier modÃ¨le n'existe pas: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
